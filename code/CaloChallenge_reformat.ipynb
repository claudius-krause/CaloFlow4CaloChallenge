{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd90290b",
   "metadata": {},
   "source": [
    "# CaloChallenge dataset reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f24214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from HighLevelFeatures import HighLevelFeatures as HLF\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42682718",
   "metadata": {},
   "source": [
    "# Dataset 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95bed15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of HighLevelFeatures class to handle geometry based on binning file\n",
    "HLF_1_photons = HLF('photon', filename='binning_dataset_1_photons.xml')\n",
    "HLF_1_pions = HLF('pion', filename='binning_dataset_1_pions.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdac803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the .hdf5 datasets\n",
    "photon1_file = h5py.File('../dataset_1_photons_1.hdf5', 'r')\n",
    "photon2_file = h5py.File('../dataset_1_photons_2.hdf5', 'r')\n",
    "pion_file = h5py.File('../dataset_1_pions_1.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60bba194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photon_1:\n",
      "dataset name:  incident_energies\n",
      "dataset shape: (121000, 1)\n",
      "dataset name:  showers\n",
      "dataset shape: (121000, 368)\n",
      "\n",
      "\n",
      "photon_2:\n",
      "dataset name:  incident_energies\n",
      "dataset shape: (121000, 1)\n",
      "dataset name:  showers\n",
      "dataset shape: (121000, 368)\n",
      "\n",
      "\n",
      "pion:\n",
      "dataset name:  incident_energies\n",
      "dataset shape: (120230, 1)\n",
      "dataset name:  showers\n",
      "dataset shape: (120230, 533)\n"
     ]
    }
   ],
   "source": [
    "# each file contains one dataset for the incident energy and one for the showers.\n",
    "print(\"photon_1:\")\n",
    "for dataset in photon1_file:\n",
    "    # name of the datasets:\n",
    "    print(\"dataset name: \", dataset)\n",
    "    print(\"dataset shape:\", photon1_file[dataset][:].shape)\n",
    "print('\\n')\n",
    "print(\"photon_2:\")\n",
    "for dataset in photon2_file:\n",
    "    # name of the datasets:\n",
    "    print(\"dataset name: \", dataset)\n",
    "    print(\"dataset shape:\", photon2_file[dataset][:].shape)\n",
    "print('\\n')\n",
    "print(\"pion:\")\n",
    "for dataset in pion_file:\n",
    "    # name of the datasets:\n",
    "    print(\"dataset name: \", dataset)\n",
    "    print(\"dataset shape:\", pion_file[dataset][:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749721c0",
   "metadata": {},
   "source": [
    "<font size=\"5\">__Photon_1:__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e983b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of events:  121000\n",
      "Number of alpha bins per layer:  [1, 10, 10, 1, 1]\n",
      "r_edges:  [[0.0, 5.0, 10.0, 30.0, 50.0, 100.0, 200.0, 400.0, 600.0], [0.0, 2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 15.0, 20.0, 30.0, 40.0, 50.0, 70.0, 90.0, 120.0, 150.0, 200.0], [0.0, 2.0, 5.0, 10.0, 15.0, 20.0, 25.0, 30.0, 40.0, 50.0, 60.0, 80.0, 100.0, 130.0, 160.0, 200.0, 250.0, 300.0, 350.0, 400.0], [0.0, 50.0, 100.0, 200.0, 400.0, 600.0], [0.0, 100.0, 200.0, 400.0, 1000.0, 2000.0]]\n",
      "Number of calorimeter layers:  5\n"
     ]
    }
   ],
   "source": [
    "# save total number of events in dataset_1_photons_1.hdf5\n",
    "n_events = photon1_file[\"incident_energies\"].shape[0]\n",
    "print('Total number of events: ',n_events)\n",
    "\n",
    "# save number of alpha bins in each layer\n",
    "num_alpha = HLF_1_photons.num_alpha\n",
    "print('Number of alpha bins per layer: ', num_alpha)\n",
    "\n",
    "#save the r edges in each layer\n",
    "r_edges = HLF_1_photons.r_edges\n",
    "print('r_edges: ', r_edges)\n",
    "\n",
    "#save total number of calorimeter layers\n",
    "n_layers = len(num_alpha)\n",
    "print('Number of calorimeter layers: ',n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d7b838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of radius bins per layer:  [8, 16, 19, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# save number of r bins per layer\n",
    "r_bins = []\n",
    "for layer in range(n_layers):\n",
    "    r_bins.append(len(r_edges[layer])-1)\n",
    "print('Number of radius bins per layer: ', r_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cf4c953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer edges:  [8, 160, 190, 5, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save layer edges\n",
    "layer_edges = []\n",
    "for layer in range(n_layers):\n",
    "    layer_edges.append(num_alpha[layer]*r_bins[layer])\n",
    "print('Layer edges: ',layer_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccfdc9",
   "metadata": {},
   "source": [
    "<font color='blue' size=3>Next, we reformat the dataset to have the structure: 'energies', 'layer_0', 'layer_1',...,'layer_4'. Note that 'energies' has format (n_events,1), while 'layer_i' has format (n_events,num_alpha_i*r_bins_i)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5ae7d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energies shape:  (121000, 1)\n",
      "layer_0 shape:  (121000, 8)\n",
      "layer_1 shape:  (121000, 160)\n",
      "layer_2 shape:  (121000, 190)\n",
      "layer_3 shape:  (121000, 5)\n",
      "layer_4 shape:  (121000, 5)\n"
     ]
    }
   ],
   "source": [
    "# format 'energies' and store incident energies\n",
    "energies = photon1_file[\"incident_energies\"]\n",
    "print('energies shape: ',energies.shape)\n",
    "\n",
    "#format 'layer_0' and store energies\n",
    "layer_0 = photon1_file[\"showers\"][:,:layer_edges[0]]\n",
    "print('layer_0 shape: ',layer_0.shape)\n",
    "\n",
    "#format 'layer_1' and store energies\n",
    "layer_1 = photon1_file[\"showers\"][:,(layer_edges[0]):(layer_edges[0]+layer_edges[1])]\n",
    "print('layer_1 shape: ',layer_1.shape)\n",
    "\n",
    "#format 'layer_2' and store energies\n",
    "layer_2 = photon1_file[\"showers\"][:,(layer_edges[0]+layer_edges[1]):(layer_edges[0]+layer_edges[1]+layer_edges[2])]\n",
    "print('layer_2 shape: ',layer_2.shape)\n",
    "\n",
    "#format 'layer_3' and store energies\n",
    "layer_3 = photon1_file[\"showers\"][:,(layer_edges[0]+layer_edges[1]+layer_edges[2]):(layer_edges[0]+layer_edges[1]+layer_edges[2]+layer_edges[3])]\n",
    "print('layer_3 shape: ',layer_3.shape)\n",
    "\n",
    "#format 'layer_4' and store energies\n",
    "layer_4 = photon1_file[\"showers\"][:,(layer_edges[0]+layer_edges[1]+layer_edges[2]+layer_edges[3]):(layer_edges[0]+layer_edges[1]+layer_edges[2]+layer_edges[3]+layer_edges[4])]\n",
    "print('layer_4 shape: ',layer_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3114f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reformatted datafile\n",
    "filename = './reformatted_data/1_gamma.hdf5'\n",
    "save_file = h5py.File(filename, 'w')\n",
    "\n",
    "save_file.create_dataset('layer_0', data=layer_0)\n",
    "save_file.create_dataset('layer_1', data=layer_1)\n",
    "save_file.create_dataset('layer_2', data=layer_2)\n",
    "save_file.create_dataset('layer_3', data=layer_3)\n",
    "save_file.create_dataset('layer_4', data=layer_4)\n",
    "save_file.create_dataset('energy', data=energies)\n",
    "    \n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7acaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
